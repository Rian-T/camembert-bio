==========================================
SLURM_JOB_ID = 1807470
SLURM_JOB_NODELIST = gpu005
==========================================
### Launching Experiment - CamemBERT-bio ###
Installing dependencies from lock file

Package operations: 1 install, 0 updates, 0 removals

  • Installing torch (2.1.0)

Installing the current project: camembert-bio (0.1.0)
Collecting torch==2.0.1
  Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)
Requirement already satisfied: filelock in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (3.12.4)
Requirement already satisfied: typing-extensions in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (4.8.0)
Requirement already satisfied: sympy in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (1.12)
Requirement already satisfied: networkx in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (3.2)
Requirement already satisfied: jinja2 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (3.1.2)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (11.7.99)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (11.7.99)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (11.7.101)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (8.5.0.96)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (11.10.3.66)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (10.9.0.58)
Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (10.2.10.91)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (11.4.0.1)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (11.7.4.91)
Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (2.14.3)
Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from torch==2.0.1) (11.7.91)
Collecting triton==2.0.0 (from torch==2.0.1)
  Using cached triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)
Requirement already satisfied: setuptools in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (68.2.2)
Requirement already satisfied: wheel in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.38.4)
Requirement already satisfied: cmake in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from triton==2.0.0->torch==2.0.1) (3.27.7)
Requirement already satisfied: lit in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from triton==2.0.0->torch==2.0.1) (17.0.3)
Requirement already satisfied: MarkupSafe>=2.0 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from jinja2->torch==2.0.1) (2.1.3)
Requirement already satisfied: mpmath>=0.19 in /home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages (from sympy->torch==2.0.1) (1.3.0)
Installing collected packages: triton, torch
  Attempting uninstall: triton
    Found existing installation: triton 2.1.0
    Uninstalling triton-2.1.0:
      Successfully uninstalled triton-2.1.0
  Attempting uninstall: torch
    Found existing installation: torch 2.1.0
    Uninstalling torch-2.1.0:
      Successfully uninstalled torch-2.1.0
Successfully installed torch-2.0.1 triton-2.0.0
/home/rtouchen/camembert-bio/camembert_bio/main.py:11: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="config", config_name="default")
[2023-10-26 16:21:46,011][HYDRA] Launching 1 jobs locally
[2023-10-26 16:21:46,011][HYDRA] 	#0 : model=per_class dataset=quaero_medline_bigbio_kb model.pretrained_model_name=Dr-BERT/DrBERT-7GB
/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|          | 0.00/45.7k [00:00<?, ?B/s]Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 22.2MB/s]                    
2023-10-26 16:21:47 INFO: Downloading default packages for language: fr (French) ...
[2023-10-26 16:21:47,776][stanza][INFO] - Downloading default packages for language: fr (French) ...
2023-10-26 16:21:49 INFO: File exists: /home/rtouchen/stanza_resources/fr/default.zip
[2023-10-26 16:21:49,299][stanza][INFO] - File exists: /home/rtouchen/stanza_resources/fr/default.zip
2023-10-26 16:21:54 INFO: Finished downloading models and saved to /home/rtouchen/stanza_resources.
[2023-10-26 16:21:54,655][stanza][INFO] - Finished downloading models and saved to /home/rtouchen/stanza_resources.
2023-10-26 16:21:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES
[2023-10-26 16:21:54,656][stanza][INFO] - Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|          | 0.00/45.7k [00:00<?, ?B/s]Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 21.8MB/s]                    
2023-10-26 16:21:54 WARNING: Language fr package default expects mwt, which has been added
[2023-10-26 16:21:54,769][stanza][WARNING] - Language fr package default expects mwt, which has been added
2023-10-26 16:21:54 INFO: Loading these models for language: fr (French):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| mwt       | combined |
========================

[2023-10-26 16:21:54,773][stanza][INFO] - Loading these models for language: fr (French):
========================
| Processor | Package  |
------------------------
| tokenize  | combined |
| mwt       | combined |
========================

2023-10-26 16:21:54 INFO: Using device: cuda
[2023-10-26 16:21:54,816][stanza][INFO] - Using device: cuda
2023-10-26 16:21:54 INFO: Loading: tokenize
[2023-10-26 16:21:54,816][stanza][INFO] - Loading: tokenize
2023-10-26 16:21:57 INFO: Loading: mwt
[2023-10-26 16:21:57,645][stanza][INFO] - Loading: mwt
2023-10-26 16:21:57 INFO: Done loading processors!
[2023-10-26 16:21:57,652][stanza][INFO] - Done loading processors!
[2023-10-26 16:22:11,127][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /tmp/tmp1wbygrfj
[2023-10-26 16:22:11,128][torch.distributed.nn.jit.instantiator][INFO] - Writing /tmp/tmp1wbygrfj/_remote_module_non_scriptable.py
Some weights of CamembertModel were not initialized from the model checkpoint at Dr-BERT/DrBERT-7GB and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/lightning_fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python camembert_bio/main.py model=per_class dataset=quaero ...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python camembert_bio/main.py model=per_class dataset=quaero ...
wandb: Currently logged in as: rntc (clinical-dream-team). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in ./wandb/run-20231026_162216-f75vv31z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run PerClass_Dr-BERT/DrBERT-7GB_quaero_medline_bigbio_kb
wandb: ⭐️ View project at https://wandb.ai/clinical-dream-team/camembert_bio
wandb: 🚀 View run at https://wandb.ai/clinical-dream-team/camembert_bio/runs/f75vv31z
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name                  | Type           | Params
---------------------------------------------------------
0 | bert                  | CamembertModel | 110 M 
1 | common_layer          | Linear         | 590 K 
2 | dropout               | Dropout        | 0     
3 | activation            | ReLU           | 0     
4 | prediction_layers     | ModuleList     | 48.4 K
5 | representation_layers | ModuleList     | 12.4 M
---------------------------------------------------------
123 M     Trainable params
0         Non-trainable params
123 M     Total params
494.654   Total estimated model params size (MB)
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.
Error executing job with overrides: ['model=per_class', 'dataset=quaero_medline_bigbio_kb', 'model.pretrained_model_name=Dr-BERT/DrBERT-7GB']
Traceback (most recent call last):
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 581, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 990, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1034, in _run_stage
    self._run_sanity_check()
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1063, in _run_sanity_check
    val_loop.run()
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py", line 181, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 134, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 391, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 403, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtouchen/camembert-bio/camembert_bio/models/nested_ner_bert.py", line 89, in validation_step
    all_preds = [self.logits_to_tags(logit, self.id2label) for logit in logits]
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtouchen/camembert-bio/camembert_bio/models/nested_ner_bert.py", line 89, in <listcomp>
    all_preds = [self.logits_to_tags(logit, self.id2label) for logit in logits]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtouchen/camembert-bio/camembert_bio/models/nested_ner_bert.py", line 79, in logits_to_tags
    tag_ids = torch.argmax(logits, dim=-1).cpu().numpy()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/rtouchen/camembert-bio/camembert_bio/main.py", line 49, in main
    trainer.fit(model, data_module)
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in fit
    call._call_and_handle_interrupt(
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1013, in _teardown
    self.strategy.teardown()
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py", line 528, in teardown
    self.lightning_module.cpu()
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/lightning_fabric/utilities/device_dtype_mixin.py", line 79, in cpu
    return super().cpu()
           ^^^^^^^^^^^^^
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/rtouchen/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
                                 ^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 🚀 View run PerClass_Dr-BERT/DrBERT-7GB_quaero_medline_bigbio_kb at: https://wandb.ai/clinical-dream-team/camembert_bio/runs/f75vv31z
wandb: ️⚡ View job at https://wandb.ai/clinical-dream-team/camembert_bio/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDEzNTI2Ng==/version_details/v1
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20231026_162216-f75vv31z/logs
                                                                   